---
title: "Bristol's Air Quality in the COVID-19 Lockdown"
author: "Steve Crawshaw"
date: "`r Sys.Date()`"
output:
  html_document: default
fig_caption: yes
editor_options:
  chunk_output_type: console
params:
  update:
    label: "Re - run Analysis?"
    value: FALSE #re-run analysis or source from file
    input: select
    choices: [TRUE, FALSE]
  dateTo:
    label: "Date of latest Monitoring data"
    value: "2020-10-31"
    input: date
---

```{r 'setup', include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      include = TRUE,
                      eval = TRUE,
                      warning = FALSE,
                      message = FALSE)

```
This brief report summarises the air quality impact of the lockdown policy on air quality measurements in Bristol. 

```{r 'load_libraries', include = FALSE}
#-----------------------------------LIBRARIES-----------------------------------
#rmarkdown::render("covid-19-air_quality_Bristol_REPORT.rmd")
#run document - objects into global env. RUN FROM CONSOLE
wants <- c("tidyverse", "data.table", "DT", "openair", "here", "lubridate", "deweather", "httr", "jsonlite", "ggeasy", "zoo", "DBI", "dbplyr", "sf", "config", "patchwork", "gganimate", "gifski", "glue", "scales", "gt")

loadLibs <- function(wants){
  suppressWarnings(suppressMessages(library(wants, character.only=T, quietly = T)))
}

has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
hold <- lapply(wants, require, character.only = T, quietly = T)
rm(hold)
```

```{r 'set_variables'}
#---------------------------------VARIABLES-------------------------------------
apikey = "9ead54a2d8a6781f3169d53bd94d99d10b6b9ff932b8a1b9852da434"
Sys.setenv(TZ = "Etc/GMT-0")
dateFrom = "2017-12-31" # for analysis of raw data
rawdata_startDate_chr <- "2018-12-31"
rawdata_startDate <- as.Date(rawdata_startDate_chr)
studyStartDate_chr <- "2020-03-01"
studyStartDate <- as.Date(studyStartDate_chr)
studyStartDate_julian = yday(studyStartDate)
dateFrom_dw <- "2020-01-01" #start of deweather analysis period
dateTo = "2020-10-31" #final day of lockdown period (or latest date for monitoring data)
#dateTo <- as.character(params$dateTo)
lockdown_start_date_chr <- "2020-03-24" #announced 23/03/2020
startDate_2020 <- as.Date(dateFrom_dw)
startdate <- as.Date(dateFrom)
enddate <- as.Date(dateTo)
startDate_dw <- as.Date(dateFrom_dw)
lockdown_start_date <- as.Date(lockdown_start_date_chr)
lockdown_end_date <- as.Date(dateTo)
lockdown_start_julian <- yday(lockdown_start_date)
lockdown_end_julian <- yday(lockdown_end_date)
valid_hours <- as.integer((enddate - startDate_dw) * 24)
ldstart_2019 <- as.Date("2018-12-31") + lockdown_start_julian
ldend_2019 <- as.Date("2018-12-31") + lockdown_end_julian
#data frame for the rectangles to be plotted as geom_rect
rect_df <- data.frame(xmin = c(as.Date("2020-03-16"), lockdown_start_date), xmax = c(lockdown_start_date, as.Date("2020-06-01")), ymin = -Inf, ymax = Inf, fill = c("Social Distancing", "Lockdown")) %>%
  mutate(fill = fct_reorder(as_factor(fill), xmin))

rdf_P <- rect_df %>% 
  mutate_if(is.Date, as_datetime) #for chart with datetime x axis

lockdown_shading <- function(rect_df){
  geom_rect(data = rect_df, mapping = aes(x = NULL, y = NULL, xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = fill), alpha = 0.3, color = "black", lty = "dashed")
}

update <- FALSE
#update <- params$update #is the analysis updating with new data?
#update is used later to set whether data retrieved from online or local data used for speed

# chart_title_date_phrase <- paste0("for period between")
source(here::here("gg_themes.r")) # custom themes for ggplots
#----------------------------WARDS BOUNDARIES MAP--------------------------------------
# wards <- read_sf("https://opendata.bristol.gov.uk/explore/dataset/wards/download/?format=geojson&timezone=Europe/London")
# 
# boundary <- read_sf("https://opendata.bristol.gov.uk/explore/dataset/bristol/download/?format=geojson&timezone=Europe/London&lang=en")
```

```{r 'Download-process-continuous'}
if(update){
#----------------------------DIFFUSION TUBES MONTHLY AND LOCATIONS---------------
  getRawTubeData <- function(apikey, dateOn, dateOff){
    base_url <- "https://opendata.bristol.gov.uk/api/v2/catalog/datasets/no2-tubes-raw/exports/csv/"
    
    yearOn <- year(as.Date(dateOn))
    yearOff <- year(as.Date(dateOff))
select_qry <- "siteid, mid_date, concentration as mean_no2, month(mid_date) as month, year(mid_date) as year"

where_qry <- glue("mid_date IN [date'",{yearOn}, "' TO date'", {yearOff}, "']")

qry_list <- list(select = select_qry, where = where_qry, apikey = apikey)

r <- GET(url = base_url, query = qry_list) #response object

if (!http_error(r)){ #extract the aggregate monthly data
   c <- content(r, as = "text") %>% 
    read_delim(delim = ";")
}
return(c)
  }
  
  monthly_tubes <- getRawTubeData(apikey = apikey, dateOn = "2011-01-01", dateOff = dateTo)
#--------------------------CONTINUOUS DATA AURN--------------------------------
core_urb_traff_sites <- c("birr", "br11", "gla4", "led6", "nca3", "nwbv", "shbr", "shlw", "burw")

core_means_joined <- importAURN(site = core_urb_traff_sites, year = 2017:2020, pollutant = "all") %>%
  filter(between(yday(date), lockdown_start_julian, lockdown_end_julian)) %>%
      group_by(site, code, year = year(date)) %>%
  summarise(no2 = mean(no2, na.rm = T),
            pm10 = mean(pm10, na.rm = T),
            pm2.5 = mean(pm2.5, na.rm = T)) %>%
  ungroup() %>%
  mutate(site = factor(ifelse(code == "BURW", "Manchester", as.character(site))),
         site = factor(ifelse(code == "SHLW", "Liverpool", as.character(site))),
         City := gsub('([A-z]+) .*', '\\1', site)) #extract all before space
#----------------------------------MET DATA------------------------------
colnames <- c("date_time", "ws", "wd", "temp")
met_data_DT <- fread(paste0("https://opendata.bristol.gov.uk/explore/dataset/met-data-bristol-lulsgate/download/?format=csv&q=date_time:%5B", rawdata_startDate_chr, "T00:00:00Z+TO+", dateTo, "T22:59:59Z%5D&timezone=UTC&lang=en&use_labels_for_header=false&csv_separator=%3B"), select = colnames)
#----------------------------AQ MONITORS META DATA----------------------------
#get site classifications for current sites

keepcols <- c("siteid", "locationclass", "location")
aq_monitors <- fread("https://opendata.bristol.gov.uk/explore/dataset/air-quality-monitoring-sites/download/?format=csv&disjunctive.pollutants=true&refine.current=True&refine.pollutants=NOX&refine.pollutants=PM10&refine.pollutants=PM2.5&refine.pollutants=O3&timezone=Europe/London&lang=en&use_labels_for_header=false&csv_separator=%3B", select = keepcols) %>% setkey(siteid)

#--------------------------------CONTINUOUS DATA ODS ---------------------------
source(here::here("importODS.r"))
aq_data_DT <- importODSAQ(
    siteid = "all",
    pollutant = c("pm10", "no2", "nox", "no", "o3", "pm25"),
    dateFrom = dateFrom,
    dateTo = dateTo
) %>% setDT()

#------------------------------EXTRACT AND PROCESS TRAFFIC DATA--------------------
#get traffic and journey speed data, join them to a table of the monitoring sites
#group and calculate summary stats, plot

scoot_aq_sites <- read_csv(paste0("S:\\SUSTAIN\\EnvQual\\Air_Quality\\Projects\\R Projects\\airquality_GIT\\data\\scoot_aq.csv"), col_types = cols(LinkID = col_character(), siteid = col_integer(), sk_dim_countdeviceid = col_character(),   sk_dim_journeylinkid = col_character()))

#----------COUNT DATA

count_sitestring <- paste0("sk_dim_countdeviceid = ", pull(scoot_aq_sites, sk_dim_countdeviceid), collapse = " OR ")

select_str_tc <- "sk_dim_countdeviceid, countsitefullname, SUM(hourlyflow) as flow"
groupby_str_tc <-  "sk_dim_countdeviceid, countsitefullname, date_format(rollupdatetime, 'YYYY-MM-dd') as date"
where_str_tc <- paste0("rollupdatetime IN ['2019-01-01T00:00:00' TO '", dateTo, "T00:00:00'] AND (", count_sitestring, ")")
base_url_tc <- "https://opendata.bristol.gov.uk/api/v2/catalog/datasets/fact-traffic-counts/aggregates/"

qry_list_tc <- list(select = select_str_tc, group_by = groupby_str_tc, where = where_str_tc, apikey = apikey)# apikey is in the query not in an authenticate argument

r_tc <- GET(url = base_url_tc, query = qry_list_tc)

count_data <- content(r_tc, as="text") %>% 
    fromJSON() %>% 
    `[[`("aggregations") %>% 
    inner_join(scoot_aq_sites, by = "sk_dim_countdeviceid") %>% 
    group_by(siteid, site, date) %>% 
    summarise(sum_daily_flow = sum(flow, na.rm = T)) 

#bulk_day_count <- readRDS(here::here("data", "bulk_day_count.rds"))

journey_sitestring <- paste0("sk_dim_journeylinkid = ", unique(pull(scoot_aq_sites, sk_dim_journeylinkid)), collapse = " OR ")

select_str_jl <- "sk_dim_journeylinkid, AVG(weightedavgspeed) as avg_speed, AVG(weightedjt) as avg_jt, journeylinkdescription"
group_by_str_jl <- "sk_dim_journeylinkid, journeylinkdescription ,date_format(rollupdatetime, 'YYYY-MM-dd') as date"
where_str_jl <- paste0("rollupdatetime IN ['2019-01-01T00:00:00' TO '", dateTo, "T00:00:00'] AND (", journey_sitestring, ")")
base_url_jl <- "https://opendata.bristol.gov.uk/api/v2/catalog/datasets/fact-journey-hourly/aggregates/"

qry_list_jl <- list(select = select_str_jl, group_by = group_by_str_jl, where = where_str_jl, apikey = apikey)

r_jl <- GET(url = base_url_jl, query = qry_list_jl)

journey_link_data <- content(r_jl, as="text") %>% 
    fromJSON() %>% 
    `[[`("aggregations") %>% 
    inner_join(scoot_aq_sites, by = "sk_dim_journeylinkid") %>% 
    group_by(siteid, site, date) %>% 
    summarise(mean_speed = mean(avg_speed, na.rm = T))

counts_speeds <- count_data %>% 
  left_join(journey_link_data, by = c("siteid" = "siteid", "date" = "date")) %>%
  mutate(date = ymd(date)) %>%
  filter(date > as.Date("2020-02-15")) %>% 
  select(-site.y) %>% 
  rename(Site = site.x, Date = date) %>% 
  pivot_longer(cols = sum_daily_flow:mean_speed, names_to = "parameter") %>% 
  mutate(parameter = ifelse(parameter == "mean_speed", "Speed (mph)", "Daily Flow (vehicles)")) %>% 
  filter(!is.na(value))

} else {

load(file = "S:/SUSTAIN/EnvQual/Air_Quality/Projects/R Projects/air_quality_data_management/data/dfs.RData") #all the data needed for the analysis
}

#-----------------------TUBE DATA PLOTTING-------------------------
# monthly_tube_plot <- monthly_tubes %>%
#     mutate(m = as.integer(month),
#            y = as.factor(year)) %>%
#     add_count(siteid, year) %>% 
#   filter(n >= 10, m %in% 4:9) %>% 
#   mutate(monthname = month.abb[m]) %>% 
#   ggplot(aes(x = y, y = mean_no2, group = y)) +
#   geom_boxplot(fill = "bisque", lwd = 1) +
#   labs(x = "Year", y = quickText("NO2 ugm-3"), title = quickText("Mean monthly NO2 for diffusion tubes 2011 - 2020"), caption = "Unbiased, raw data, not for comparison with annual objectives") +
#   facet_grid(rows = vars(fct_reorder(monthname, m))) +
#  theme_ppt_single() +
#   theme(panel.grid = element_blank())

monthly_tube_plot <- monthly_tubes %>%
  add_count(siteid, year) %>% 
  filter(year >=2019, n >= 10) %>% 
  group_by(month, year) %>% 
  summarise(month_year_mean = mean(mean_no2, na.rm = T), .groups = "keep") %>% 
  mutate(month = as.integer(month),
         year = as.factor(year),
         monthname = month.abb[month]) %>% 
  ggplot() +
geom_line(aes(x = fct_reorder(monthname, month), y = month_year_mean, group = month), lwd = 1, lty = 5, colour = "black") +
  geom_point(aes(x = fct_reorder(monthname, month), y = month_year_mean, colour = year), size = 5) +
  labs(x = "Month",
       y = quickText("NO2 ugm-3"),
       colour = "Year",
       title = "Difference between 2019 and 2020 monthly diffusion tube means",
       caption = "Unbiased, raw data, not for comparison with annual objectives") + theme_ppt_single() +
   theme(panel.grid.major = element_blank(),
         panel.grid.major.y = element_line(colour = "gray"),
         panel.grid.minor.y = element_blank())

#-------------------------------------PM10 SOURCE APP CALCS_---------------

pm10_am_url <- "https://opendata.bristol.gov.uk/api/v2/catalog/datasets/air-quality-data-continuous/aggregates?select=avg(pm10)%20as%20mean_pm10&group_by=year(date_time)%2C%20siteid&where=date_time%3E%3D'2019'%20AND%20date_time%3C%3D'2019'"
#get annual mean PM10 data from ODS using API V2 and use later in text
pm10_am_data <- pm10_am_url %>% 
  GET() %>% 
  content(as = "text") %>% 
  fromJSON %>% 
  `[[`(2) %>% #get second element of list
  filter(!is.na(mean_pm10))# remove non pm10 sites

pm10_roadside_inc <- (max(pm10_am_data$mean_pm10) - min(pm10_am_data$mean_pm10))/ max(pm10_am_data$mean_pm10) * 100 #percentage of PM10 which is roadside increment - BTW \ AURN

```

```{r 'data-wrangling'}
#---------------------------------BCC--------------------------------
lockdown_long <- aq_data_DT %>% 
  filter(date >= rawdata_startDate,
    between(yday(date), lockdown_start_julian, lockdown_end_julian)) %>% 
  mutate(year = as.numeric(format(date, "%Y"))) %>% 
  inner_join(aq_monitors[, 1:2], by = "siteid") %>% 
  pivot_longer(cols = pm10:pm2.5, names_to = "Pollutant", values_to = "Concentration") %>% 
  mutate(Pollutant = as.factor(toupper(Pollutant))) %>% 
  na.omit(cols = "Concentration") %>% 
  mutate(ld_day = yday(date) + 2 - lockdown_start_julian)

lockdown <- lockdown_long %>% 
  mutate(Pollutant = as.factor(tolower(Pollutant))) %>%
  pivot_wider(names_from = Pollutant, values_from = Concentration)

lockdown_roadside <- lockdown %>% 
  filter(locationclass %in% "Urban Traffic") %>% 
  mutate(location = ifelse(location == "Parson Street School", "Parson St.", location)) 
#for chart text fit

#--------------------------------MET DATA-------------------------
met_data_DT_hr <- met_data_DT %>% 
  mutate(date = as.POSIXct(date_time, format = "%Y-%m-%dT%H:%M"),
         year = as.factor(str_sub(date, 1, 4))) %>% 
  timeAverage(avg.time = "hour") %>% setDT()

#-----------------------DE WEATHER WRANGLING------------------------------
#hour average met data, join to AQ data , join to aq_monitors
#add variables for modelling
#strip NA values for no2 and pm10
#filter the sites for data capture rates < 0.9

aq_met_dw_hourly <- aq_data_DT[met_data_DT_hr, on = "date"][aq_monitors, on = "siteid"] %>% 
                        prepData(add = c("hour", "hour.local", "weekday", "trend",
                                     "week", "jday", "month"), local.tz = "Europe/London", lag = NULL) %>% 
  .[, site := as.factor(location)] %>% 
  .[date >= startDate_dw] %>% #only 2020 data
  filter(!is.na(no2) | !is.na(pm10) | !is.na(nox)) %>% #select pollutants
  add_count(site, name = "count") %>% 
  filter(count / valid_hours > 0.9) %>% #View()#remove sites with low data capture
  mutate(site = droplevels(site), 
         locationclass = as.factor(locationclass),
         dw_group = case_when(
           siteid == 501 ~ "City Centre",
           siteid != 501 & locationclass == "Urban Traffic" ~ "Urban Traffic",
           locationclass == "Urban Background" ~ "Urban Background"
         )) #classify sites into 3 classes for intelligible plotting

#aggregate by date and dw_group to give mean hourly values for all pollutants
dw_hourly_agg <- aq_met_dw_hourly %>% 
  group_by(date, dw_group, weekday, month) %>% 
  summarise_if(is.numeric, mean, na.rm = T)

#set variables for the models
vars = c("trend", "ws", "wd", "hour", "weekday", "temp", "week")
metVars = c("ws", "wd", "hour", "weekday", "temp", "week")

#pivot to enable grouping by pollutant
#keep only pm10 and no2 as model fails with low numbers from pm2.5 and ozone
#group by pollutant and site type, nesting other columns into a list column, data
#apply the testMod, buildMod, and metSim models to each data frame by group
#also create a ggplot object for each de - weathered output and a filename so that they can be iteratively plotted

#-------------run the deweather model for grouped data------------------
if(update){
models_plots <- dw_hourly_agg %>%
  pivot_longer(cols = pm10:pm2.5, names_to = "pollutant", values_to = "concentration") %>%
  filter(pollutant == "pm10" | pollutant == "no2" | pollutant == "nox") %>%
  group_by(dw_group, pollutant) %>%
  nest() %>%
  mutate(testMod = map(data, ~ testMod(., vars = vars, pollutant = "concentration")),
         buildMod = map(data, ~ buildMod(., vars = vars, pollutant = "concentration", n.core = 6, B = 100)),
         demet = map(buildMod, ~ metSim(., metVars = metVars)),
         plot = map(demet, ~ggplot(timeAverage(., "day"), aes(date, pred)) + geom_line(col = "dodgerblue", size = 1) +
    ylab(quickText("ug/m3")) +
    labs(title = quickText(paste0("De - weathered concentrations of ", pollutant, ": ", dw_group)))),
    filename = paste0(dw_group, "_", pollutant, "_", ".png"))
#-------------------------------------run models by site--------------------

models_plots_sites <- aq_met_dw_hourly %>%
  pivot_longer(cols = pm10:pm2.5, names_to = "pollutant", values_to = "concentration") %>%
  filter(pollutant == "no2" | pollutant == "nox") %>%
  group_by(site, pollutant) %>%
  nest() %>% #View()
  mutate(testMod = map(data, ~ testMod(., vars = vars, pollutant = "concentration")),
         buildMod = map(data, ~ buildMod(., vars = vars, pollutant = "concentration", n.core = 6, B = 100)),
         demet = map(buildMod, ~ metSim(., metVars = metVars)),
         plot = map(demet, ~ggplot(timeAverage(., "day"), aes(date, pred)) + geom_line(col = "dodgerblue", size = 1) +
    ylab(quickText("ug/m3")) +
    labs(title = quickText(paste0("De - weathered concentrations of ", pollutant, ": ", site)))),
    filename = paste0(site, "_", pollutant, "_", ".png"))

save(aq_data_DT, met_data_DT, aq_monitors, core_means_joined, models_plots, models_plots_sites, monthly_tubes, counts_speeds, file = "S:/SUSTAIN/EnvQual/Air_Quality/Projects/R Projects/air_quality_data_management/data/dfs.RData")

}

#-------deweathered predictions for site CLASSES------------------
#extract (unnest) the dataframe of predictions from the deweather model
deweathered_predictions <- models_plots %>% 
  select(dw_group, pollutant, demet) %>% 
  unnest(demet)

nox_no2_2020 <- aq_data_DT %>% 
  select(date, site = location, nox, no2) %>% 
  filter(date >= startDate_dw) %>% 
  pivot_longer(cols = nox:no2, names_to = "pollutant", values_to = "concentration") %>% 
  group_by(site, pollutant) %>% 
  summarise(mean = mean(concentration, na.rm = T), .groups = "drop")

fbracket <- function(x) str_replace(x, "\\(", "\\[") # function to change weird formatting of brackets when cut_number bins the continuous data

dwp_site_data <- models_plots_sites %>% 
  select(site, pollutant, demet) %>% 
  unnest(demet) %>%
  ungroup() %>% 
  mutate(site = as.character(site)) %>% 
  inner_join(nox_no2_2020, by = c("site" = "site", "pollutant" = "pollutant")) %>%
  timeAverage(avg.time = "day", pollutant = "delta", type = c("site", "pollutant")) %>%
  ungroup() %>% 
  mutate(delta = ((pred - mean) / mean) * 100,
         delta_binned = fct_relabel(cut_number(round(delta), n = 5), fbracket),#relabel with function
         pollutant = toupper(pollutant)) %>% 
  filter(!is.na(delta_binned)) 

saveRDS(dwp_site_data, file = here::here("data", "dwp_site_data.rds"))

#tabulate summary stats for the deweathered data pre and post lockdown
dwp_sum_stats <- dwp_site_data %>%
  mutate(lockdown = if_else(date >= lockdown_start_date, "lockdown", "pre - lockdown")) %>% 
  group_by(site, pollutant, lockdown) %>% 
  summarise(mean_pred = mean(pred, na.rm = T)) %>% 
  pivot_wider(id_cols = site, names_from = c("lockdown", "pollutant"), values_from = mean_pred) %>% 
  mutate(`NO2 Change %` = ((lockdown_NO2 - `pre - lockdown_NO2`)/ `pre - lockdown_NO2`)*100,
         `NOX Change %` = ((lockdown_NOX - `pre - lockdown_NOX`)/ `pre - lockdown_NOX`)*100) %>% 
  mutate_if(is.numeric, round) %>% 
  select(site, `NO2 Change %`, `NOX Change %`)

mean_deweathered_no2_change <- mean(x = dwp_sum_stats$`NO2 Change %`, na.rm = T)

dwp_plot_site <- dwp_site_data %>% 
  ggplot(aes(x = date, y = site, fill = delta_binned)) + #use geom_tile
  geom_tile() +
  geom_vline(xintercept = as.POSIXct("2020-03-24"), lty = 5, lwd = 1) +
  facet_wrap(~ pollutant) +
  scale_color_brewer(palette = "RdYlGn", aesthetics = "fill", direction = -1) +
  labs(x = "Date", y = "Site", fill = "% difference between \n de - weathered \n prediction and \n mean for 2020", title = "Variation between de - weathered predictions \nand mean concentrations in 2020") +
  theme_report_facet() +
  theme(strip.text.x = element_text(face = "bold"),
        axis.text.y = element_text(face = "bold"),
        axis.text.x = element_text(face = "bold"),
        legend.text = element_text(size = 14))


dwp_bar_data <- dwp_site_data %>% #Weekly Bar Plot
timeAverage(avg.time = "week", type = c("site", "pollutant")) %>% 
mutate(Measures = case_when(
  date < as_datetime("2020-03-16") ~ "Pre COVID",
  date < as_datetime("2020-03-23") ~ "Social Distancing",
  date < as_datetime("2020-06-02") ~ "Lockdown",
  TRUE ~ "Post Lockdown (1)"
)) %>% 
  filter(pollutant == "NOX")

dwp_bar_data$Measures <- factor(dwp_bar_data$Measures, levels = c("Pre COVID", "Social Distancing", "Lockdown", "Post Lockdown (1)"))

dwp_bar_plot <- ggplot(data = dwp_bar_data, aes(x = date, y = pred, fill = Measures)) +
  geom_col() +
        facet_wrap(~ site) +
  theme_report_facet() +
  labs(x = "Date: 2020", y = quickText("Weekly mean concentrations:  ugm-3"), title = quickText("Weekly de - weathered NOx"), fill = "Covid Measures")
 
#and plot
dwp_plot <- deweathered_predictions %>% 
  ungroup() %>% 
  timeAverage(avg.time = "day", type = c("dw_group", "pollutant")) %>% 
  rename(`Site type` = dw_group) %>%
  ungroup() %>% 
  mutate(pollutant = toupper(pollutant)) %>% 
  ggplot(aes(x = date, y = pred, color = `Site type`))  +
  geom_rect(data = rdf_P, mapping = aes(x = NULL, y = NULL, xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = fill), alpha = 0.1, color = "black", lty = "dashed") +
  scale_fill_manual(values = c("#00AFBB", "#FC4E07")) +
  geom_line(lwd = 2) +
  scale_color_brewer(palette = "Dark2") +
  labs(title = "Daily mean de - weathered predictions", subtitle = "Bristol: 2020", caption = "Operational issues at city centre site: April 9th to April 16th", fill = "Period", y = quickText("Predicted concentration ( ugm-3 )"), x = "Date") +
  facet_grid(pollutant ~ ., scales = "free_y") +
  theme_report_facet() +
  easy_plot_legend_size(size = 14) +
  easy_plot_legend_title_size(size = 16) +
  theme(strip.text.y = element_text(color = "black", face = "bold", size = 14, angle = 0))

```

```{r 'summary-calcs'}
calc_pc_change <- function(df){
  df %>% 
  mutate(`% Change` = (`2020` -`2019`)/`2019` * 100) %>%
  mutate_if(is.numeric, round) %>% 
  filter(Pollutant != "NO")
}

mean_site_table <- lockdown_long %>% 
  group_by(location, Pollutant, year = year(date)) %>%
   summarise(mean_conc = mean(Concentration, na.rm = T)) %>% 
  pivot_wider(id_cols = location:Pollutant, names_from = year, values_from = mean_conc) %>% 
  calc_pc_change()

mean_pollutant_table <- lockdown_long %>% 
  group_by(Pollutant, year = year(date)) %>% 
  summarise(mean_conc = mean(Concentration, na.rm = T)) %>% 
  pivot_wider(id_cols = Pollutant, names_from = year, values_from = mean_conc) %>% 
  calc_pc_change() %>% setDT()

fwrite(mean_pollutant_table, here::here("data", "mean_pollutant_table.csv"))

changeno2 <- mean_pollutant_table[Pollutant == "NO2", `% Change`] #for use in the text
changenox <- mean_pollutant_table[Pollutant == "NOX", `% Change`]
changepm10 <- mean_pollutant_table[Pollutant == "PM10", `% Change`]


```

```{r 'weekly-means-by-year'}

pcChange <- function(year1, year2){
  (year2 - year1) / year1
}

weekly_compare <- lockdown_roadside %>% 
  group_by(siteid, location, year = as.factor(year), week = lubridate::week(date)) %>%
  summarise(across(pm10:o3, .fns = mean, na.rm = T), .groups = "drop") %>% 
  pivot_longer(cols = pm10:o3,
               names_to = "pollutant",
               values_to = "concentration")

weekly_change <- weekly_compare %>% 
  pivot_wider(id_cols = c("siteid", "location", "week", "pollutant"),
              names_from = year,
              names_prefix = "conc_",
              values_from = concentration) %>% 
  mutate(delta_p = pcChange(conc_2019, conc_2020)) %>%
   group_by(siteid, location, pollutant) %>%
  slice_min(delta_p, n = 1) %>% 
  transmute(siteid, location, pollutant = toupper(pollutant), week_beginning = as.Date(paste0("2020", week, 1), "%Y%U%u"), delta_p) %>% 
  ungroup() 

change_by_poll <- function(df, poll){
  df %>% 
    filter(pollutant %in% poll) %>% 
    filter(delta_p == min(delta_p))
}

#max of weekly changes
max_nox_change <- change_by_poll(weekly_change, "NOX")
max_no2_change <-   change_by_poll(weekly_change, "NO2")

weekly_change_table <- weekly_change %>% 
  filter(pollutant %in% c("NOX", "NO2")) %>%
  mutate(pollutant = ifelse(pollutant == "NO2", "NO<sub>2</sub>", "NOx")) %>% 
  gt() %>% 
   tab_style(style = cell_text(weight = "bold"),
        locations = cells_column_labels(everything())) %>% 
  cols_label(siteid = "Site ID",
             delta_p = "Change",
             location = "Site",
             pollutant = "Pollutant",
             week_beginning = "Week beginning") %>% 
  fmt_percent(columns = vars(delta_p), decimals = 0) %>%
  fmt_markdown(columns = vars(pollutant)) %>% 
  tab_header(
    title = "Maximum weekly change by pollutant and site",
    subtitle = "Compared to 2019")



#bar chart for the weekly concentrations in 2019 and 2020
weekly_chart <- weekly_compare %>%
filter(pollutant == "nox") %>%
  mutate(date = as.Date(paste0("2020", week, 1), "%Y%U%u")) %>%
  ggplot(aes(x = date, y = concentration, fill = year)) +
  geom_col(position = position_dodge(preserve = "single")) +
  facet_wrap( ~ location) +
  scale_color_brewer(palette = "Dark2") +
  # scale_fill_manual(values=c("#56B4E9", "#E69F00")) +
  scale_x_date(date_breaks = "2 months", date_labels = "%b") +
  labs(title = "Change in weekly NOx concentrations", subtitle = "2019 to 2020",
       x = "Date", y = quickText("NOx ugm-3"), fill = "Year") +
  theme_report_facet()
  
```


## Introduction
This report summarises changes to ambient air quality, represented by concentrations of regulated pollutants, that have occurred during the COVID-19 crisis. Bristol City Council continued to monitor air quality throughout lockdown as this work was classed by the government as an essential activity.

Traffic levels declined rapidly from slightly before lockdown leading to a clear reduction in emissions of key pollutants. This is apparent for roadside and background sites.

Air quality changed during the period, but the changes in air quality characterised by comparisons of *raw* data between two periods cannot be solely attributed to the lockdown measures, because weather and other variables strongly influence ambient air quality. In order to account for the influence of weather and other covariates, a statistical modelling approach has been adopted which can remove the effect of the weather and identify the changes in concentrations which would arise if meteorological conditions and temporal effects are held constant. This approach can be used to ascribe the changes in air quality to lockdown measures with more certainty than simply by comparing raw data between two periods.

## Summary of changes
The comparison of raw data between 2019 and 2020 was for the period `r ldstart_2019` to `r ldend_2019` and `r lockdown_start_date` to `r lockdown_end_date`.

Analysis of air quality data from Bristol City Council's continuous air monitoring network comparing the lockdown period in 2020 to the same period in 2019 shows a significant change in nitrogen dioxide (NO~2~), a traffic pollutant, of a maximum `r round(max_no2_change[1, 5] * 100)`% as a weekly average. Reductions in NOx (oxides of nitrogen), which can be considered a surrogate for direct exhaust emissions, fell even further with a maximum mean weekly change of `r round(max_nox_change[1, 5] * 100)`%. These changes cannot be attributed solely to the lockdown measures because of the effect of weather and the small effect of changes in the vehicle fleet between the two comparison periods.

Measures of particulate matter (PM) - PM~10~ and PM~2.5~ also fell but the reduction was less. This is because the local contribution to ambient PM is a small part of the total. There are significant regional and background components present which are unaffected by the lockdown measures. For PM~10~ the roadside increment in 2019 was `r round(pm10_roadside_inc)`% when comparing a background and roadside site in the central city.

Ozone (O~3~) rose in the first lockdown when compared to the baseline period. This is expected because as NOx declines, less ozone is chemically reduced in the photochemical reaction between these two species and hence concentrations of ozone may rise. Unusually sunny weather also contributed to the higher than usual levels of ozone.

Data from the [NO~2~ diffusion tube network](https://opendata.bristol.gov.uk/explore/dataset/no2-diffusion-tube-data/map/?disjunctive.location&refine.year=2018&location=12,51.46856,-2.60587&basemap=jawg.streets), which gives greater spatial coverage than the continuous network has been analysed. The changes in measured nitrogen dioxide for each month where we have data are shown. There is typically around a 6 week delay between the end of the month where tubes have been exposed, to receiving the results.

All of Bristol's air quality data are available through our [open data portal.](https://opendata.bristol.gov.uk/pages/air-quality-dashboard-new/map#air-quality-now)


## Traffic
Traffic flows started to decline when the first measures were announced on the 16th March. By the time full lockdown was operating (from 24th March), traffic flows had declined by more than 50% compared to normal levels. Data are from BCC's Urban Traffic Control (UTC) system and represent smoothed daily mean speeds and total flows at routes or counters close to the continuous air quality monitoring sites.

It can be seen that traffic flows began to climb slowly again after the first week in April.

```{r 'traffic-flows', out.width = '100%', fig.width = 14, fig.height = 12, fig.cap = "Fig 1. Daily traffic flows and speeds: 2020"}

speed_flow_plot <- counts_speeds %>%
  ggplot(aes(x = Date, y = value, color = Site)) +
  geom_smooth(method = "loess", se = F, lwd = 2) +
  facet_wrap(~ parameter, scales = "free_y") +
  theme_report_facet() +
  scale_color_brewer(palette = "Dark2") +
  labs(title = "Daily vehicle flows and mean speed at air quality monitoring sites", y = "Value")

speed_flow_plot

```

Google and Apple are providing access to their mobility data during the COVID-19 lockdown. The chart below shows weekly trends in [mobility by sector for Bristol from Google's dataset](https://opendata.bristol.gov.uk/explore/dataset/google_mobility_data/information/). The baseline for comparison is the median value, for the corresponding day of the week, during the 5-week period Jan 3 – Feb 6, 2020. The initial reaaduction in mobility for all sectors except residential is apparent. In this context the residential category includes deliveries to homes. There does seem to be a slight increase after the first week in April of non residential sector mobility and from mid - May there was a steep increase in mobility associated with use of parks.

```{r 'Traffic-flows-by-sector', results = "show", out.width = '100%', fig.width = 14, fig.height = 12, fig.cap = "Fig 2. Mobility by sector: 2020"}

#-------------------------GOOGLE TRAFFIC DATA----------------------
google_mob_ODS <- read_delim("https://opendata.bristol.gov.uk/explore/dataset/google_mobility_data/download/?format=csv&timezone=Europe/London&lang=en&use_labels_for_header=false&csv_separator=%3B", delim = ";")

sector_mobility_plot <-   google_mob_ODS %>%
    mutate(Sector = str_to_title(str_replace_all(sector, c("_" = " ", "and" = "&")))) %>%
  group_by(week(date), Sector) %>% 
  summarise(date = mean(date), percent_change_from_baseline = mean(percent_change_from_baseline)) %>% 
    ggplot(aes(x = date, y = percent_change_from_baseline, color = Sector)) +
     # geom_smooth(se = F, lwd = 2) +
  geom_line(lwd = 2) +
    geom_vline(xintercept = as.Date("2020-03-24"), lty = 5, lwd = 2) +
    labs(y = "% change from baseline",
         x = "Date",
        title = "Changes in Mobility by Sector",
        subtitle = "Weekly mean change",
        caption = paste0("Google COVID-19 Community Mobility Reports. \n      https://www.google.com/covid19/mobility/ Accessed: ", Sys.Date())) +
  scale_color_brewer(palette = "Dark2") +
 theme_ppt_single() +
  theme(legend.title = element_text(size = 12),
        legend.text = element_text(size = 12)) +
      scale_x_date(date_breaks = "months" , date_labels = "%b") 

sector_mobility_plot

```

## Summary of changes by pollutant

The table below shows the maximum percentage changes of weekly mean concentrations aggregated by site and pollutant. Roadside sites have been selected as these are the most affected. The analysis compares weekly averages to the same week in the preceding year, i.e. 2019. While this does not take into account the effects of the weather on concentrations, it provides an indication of the extent of change and by using weekly means, noise that would be apparent when using a daily mean value is reduced.
Bristol City Council's data are ratified according to processes detailed with our [annual status reports](https://www.bristol.gov.uk/documents/20182/32675/Bristol+City+Council+2019+Air+Quality+Annual+Status+Report+ASR.pdf/62eeb142-ac59-ac08-148b-a1247e08b1db). Data from the national network sites (Bristol St. Pauls and Bristol Temple Way) are not fully ratified at the time of writing.

```{r 'generate-mean-pollutant-table', results = "show", out.width = '100%', fig.width = 14, fig.height = 12, fig.cap = "Table 1. Maximum change in weekly mean pollutant concentrations"}

weekly_change_table
```
<br />
The chart below shows the weekly mean concentrations of NOx at sites in 2019 and 2020. Not all sites measure all pollutants. NOx is selected as this pollutant is most closely associated with traffic emissions.

```{r 'weekly-chart', out.width = '100%', fig.width = 14, fig.height = 12, fig.cap = "Fig 3. Weekly changes in mean NOx by site and year"}
weekly_chart
```
<br />

## Locations of continuous monitoring sites

```{r 'aq-map', out.width = '100%', fig.width = 14, fig.height = 12, fig.cap = "Fig 4. Locations of current continuous monitoring sites"}
knitr::include_url("https://opendata.bristol.gov.uk/explore/embed/dataset/air-quality-monitoring-sites/map/?disjunctive.pollutants&refine.current=True&refine.instrumenttype=Continuous%20(Reference)&location=12,51.45296,-2.57009&basemap=jawg.streets&static=false&datasetcard=true&scrollWheelZoom=false", height = "500px")
#embed the map of live sites from ODS
```

## Comparison with other cities

The changes in Bristol can be shown in context with other core cities by using data from the national AURN network. The plot below shows the mean values of NO~2~ for the lockdown period in each of the previous four years. Bristol's national roadside site at Temple Way was not operating in 2017.

It is not possible to infer much meaning from the relative differences in changes between cities as there are a range of site - specific factors that could influence this, but the data are presented to show the national context.

```{r 'data-viz-National', results = "show", fig.height = 7, fig.width = 11, fig.cap = "Fig 4. National comparison: nitrogen dioxide at urban traffic sites"}
#--------------------------------------CORE CITIES-------------------------
city_plot_covid_by_year <- core_means_joined %>% 
  filter(!is.na(no2)) %>% 
  ggplot(aes(x = City, y = no2, fill = as.factor(year))) +
  geom_col(position = "dodge") + 
  scale_y_continuous(quickText("NO2 ug m-3")) +
  labs(title = quickText("Mean NO2 at core city urban traffic sites in lockdown period"),
       x = "City") +
  labs(fill = "Year") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size=13)) +
  theme_ppt_single() +
  scale_color_brewer(palette = "Dark2") +
  theme(axis.text.x = element_text(angle = 90),
         panel.grid.major = element_blank()) 

city_plot_covid_by_year 

```

## Changes in air quality at Bristol's continuous monitoring sites

Bristol's city centre site at [Colston Avenue](https://opendata.bristol.gov.uk/pages/aqcontinuoussites/?q=siteid:501) is the most polluted roadside site on the network and so is an interesting example of the reductions during the lockdown. The chart below shows weekly mean concentrations of NOx, NO~2~ and PM~10~ for the baseline and lockdown period. Operational issues between 9th April and 15th April mean that data is incomplete for this period.

```{r 'data-viz-Colston', results = "show", fig.height = 7, fig.width = 11, fig.cap = "Fig 5. Chart of Colston Avenue site by pollutant"}
#plot bar chart of means by day
sites_weekly <- lockdown_long %>% 
  filter(Pollutant %in% c("NOX", "NO2", "PM10")) %>%
  mutate(siteid = as.factor(siteid)) %>% 
  timeAverage(avg.time = "week", type = c("Pollutant", "siteid")) %>% 
  ungroup() %>% 
  filter(!is.na(year))

colston <- sites_weekly %>% 
  filter(siteid == 501) %>%
  ggplot(aes(x = date, y = Concentration, fill = Pollutant)) +
    geom_col(position = "dodge") + 
    scale_y_continuous(quickText("ug m-3")) +
    labs(x = "Date", title = "Changes in weekly mean concentrations by year in lockdown periods: Colston Avenue", caption = "Operational issues 9th April to 16th April 2020") + 
scale_fill_manual(values = c("#D95F02", "#1B9E77", "#7570B3")) +
         facet_wrap(~ as.factor(year), scales = "free_x") 

colston +
  theme_report_facet()
```

Other sites on the network show similar patterns. For the [Wells Road](https://opendata.bristol.gov.uk/pages/aqcontinuoussites/?q=siteid:270) site, the difference in concentrations between the two periods was not as pronounced as for Colston Avenue.

```{r 'data-viz-Bristol-sites', results = "show", fig.height = 7, fig.width = 11, fig.cap = "Fig 6. NOx and NO2: Wells Road"}
siteplot <- sites_weekly %>% 
   filter(siteid == 270) %>%
  ggplot(aes(x = as.Date(date), y = Concentration, fill = Pollutant)) +
    geom_col(position = "dodge") +
    scale_y_continuous(quickText("ug m-3")) +
    labs(x = "Date", title = "Changes in weekly mean concentrations in lockdown periods: Wells Road") +
  scale_fill_manual(values = c("#D95F02", "#1B9E77")) +
     facet_wrap(~ as.factor(year), scales = "free_x") 

siteplot +
  theme_report_facet() 
  
```

## Changes in nitrogen dioxide at diffusion tube sites
Diffusion tubes are used to monitor ambient nitrogen dioxide. Nitrogen dioxide from the air diffuses into a metal grid in the tube and the tubes are sent for analysis in a laboratory. Diffusion tubes are placed according to a monthly calendar specified by Defra and BCC's diffusion tubes are exposed according to this calendar. April 2020 was the first complete month where tubes were entirely exposed under the lockdown measures.

The chart below shows that the difference in concentrations between 2019 and 2020 increased dramatically as the first lockdown took effect and diminished as lockdown measures were eased during the summer.

```{r 'data-viz-diffusion-tubes', warning = FALSE, results = "show", fig.height = 7, fig.width = 11, fig.cap = "Fig 7. mean concentrations of NO2 in April diffusion tubes by year"}

monthly_tube_plot
```

## Accounting for meteorology

Analysis of the wind speed and direction from [Bristol Lulsgate](https://opendata.bristol.gov.uk/explore/dataset/met-data-bristol-lulsgate/information/) show that although wind speeds from the NE quadrant were higher in the 2020 lockdown period than in the baseline period in 2019, it is unlikely that this can explain all of the dramatic reduction shown, as the wind from the dominant direction for this period will be somewhat polluted by other urban upwind emissions. Further de - weathering analysis is required to establish this with more certainty and is summarised in the following section.

```{r 'data-viz-Meteorology', results = "show", fig.height = 7, fig.width = 10, fig.cap = "Fig 8. Wind roses for lockdown periods: Bristol Lulsgate"}

met_data_DT_hr %>% 
  filter(between(yday(date), lockdown_start_julian, lockdown_end_julian)) %>% 
  windRose(ws = "ws", wd = "wd", type = "year", main = "Wind roses for COVID-19 lockdown periods: 2019 and 2020")
```

### De - weathering ambient air quality measurements
The deweather functions of the [openair package](https://cran.r-project.org/web/packages/openair/index.html) were used to remove the effect of the weather on concentrations of regulated pollutants measured by Bristol City Council's and Defra's monitoring sites in Bristol. The deweather package uses a boosted regression tree approach for modelling air quality data. This technique builds a statistical model of the air quality data and thereby takes account of the many complex interactions between variables as well as non-linear relationships between the variables.

Predictions of daily mean concentrations are derived from the modelled hourly means of data that are aggregated by site type. The accepted classifications of "Urban Background" (distant from busy road) and "Urban Traffic" (close to busy road) were used, as well as a classification of "City Centre" to cover the most polluted site, Colston Avenue, as this site represents the most polluted air in the city. Sites with a data capture less than 90% are not used in the analysis. The table below shows the classes for each of the sites analysed.

```{r 'table-sites', results = "show", fig.height = 7, fig.width = 13, fig.cap = "Table 2. Sites aggregated for model analysis"}

classtable <-   aq_met_dw_hourly %>% 
    select(site, dw_group) %>% 
    group_by(site, dw_group) %>% 
    summarise(n=n()) %>% 
    ungroup(site) %>% 
    select(-n) %>% 
  datatable(colnames = c("Site", "Classification"),
            options = list(dom = "t")) # just the table
#figure captions only seem to work for DT::datatables
  
classtable

```

The chart shown below is the predicted "de - weathered" concentrations of three pollutants grouped into site types. The removal of the influence of the weather indicates that the reductions in measurements of traffic pollutants are probably not due solely to the weather. Reductions in traffic emissions due to COVID-19 lockdown measures is the likely explanation.

For NO~2~ it is noticeable that at the city centre site (Colston Avenue) concentrations started to decline around the 16th March, and continued to decline further in the days immediately following lockdown on the 24th March. The post - lockdown decline was also apparent in the urban traffic and urban background site classes. A small increase in NO~2~ at urban traffic and urban background sites was seen around 8th - 12th of April. This could be explained by a regional pollution episode that also increased PM~10~ concentrations during the same period. Unfortunately operational issues at the city centre site mean that data was unavailable from the 9th April to the 15th April.

For NOx, a similar reduction was seen at the city centre site after the 16th April and a small reduction in concentrations at urban traffic sites is apparent.

PM~10~ concentrations did decline in the immediate post - lockdown period but then rose steeply during the pollution episode over Easter weekend. The boosted regression tree model used in the de - weathering process take into account wind speed and direction but cannot account for elevated pollutant levels in the incoming air and hence are unable to remove the effect of regional pollution episodes such as the one that occurred at this time.

```{r 'deweather-chart', results = "show", fig.height = 7, fig.width = 14, fig.show = "show", fig.cap = "Fig 8. Deweathered chart by site class"}

dwp_plot 

```

The de - weathering process can also be applied to individual sites. The chart below shows how the de-weathered daily predictions vary from the mean levels for traffic pollutants for 2020 for each site. It is clear that from the date of lockdown, significant reductions have ocurred at all sites for NOx and NO~2~ and that these reductions are not primarily driven by weather. It can be seen that deweathered concentrations start to rise from mid June.

```{r 'deweathered-predictions', results = "show", fig.height = 7, fig.width = 10, fig.show = "show", fig.cap = "Fig 9. Deweathered chart by site: NOx and NO2"}
#find the means of NOx and NO2 by site for 2020 for comparison with delta
dwp_plot_site

```

The predicted changes in the deweathered NOx and NO~2~ concentrations are summarised in the table below. The comparison is between the period `r dateFrom_dw` - `r lockdown_start_date - 1` and `r lockdown_start_date_chr` - `r dateTo`. It is clear that Colston Avenue experienced the largest drop in NO~2~ and NOx concentrations.

```{r 'deweathered-predictions-of-NOx-and-NO2-by-site-summary-stats', results = "show", fig.height = 7, fig.width = 10, fig.show = "show", fig.cap = "Table 3. Summary statistics for de - weathered NOx and NO2 concentrations"}

datatable(dwp_sum_stats, rownames = F,
              options = list(dom = 't'),
          colnames = c("Site", "NO2 Change %", "NOx Change %"))
```

Plotting the weekly mean concentrations of the traffic pollutants NOx and NO~2~ shows the effect of lockdown rules on weekly concentrations by controlling for the effect of the weather. It can be seen from the plot below that concentrations remained low throughout May despite some evidence that traffic levels are climbing since mid April. There is some evidence of rising concentrations towards the end of June, particularly at Colston Avenue.

```{r 'deweathered-weekly-NOx-and-NO2-by-site', results = "show", fig.height = 7, fig.width = 10, fig.show = "show", fig.cap = "Fig. 10. Predicted weekly mean NOx and NO2: De - weathered predictions"}

dwp_bar_plot
```

```{r 'plot-save', results = "hide", fig.show = "hide"}

if(update){ #save the plots to files for use in PPT etc
  objs =  mget(ls(envir=.GlobalEnv), envir=.GlobalEnv) # get a list of the objects in the global environmemt
ggplot_list <- Filter(function(i) inherits(i, "ggplot"), objs) #filter for class == ggplot

filenames <- paste0(names(ggplot_list), ".png") #make a list of filenames

plot_df <- data.frame(filename = filenames, plot = I(ggplot_list)) #create DF to pass to ggsave

pwalk(plot_df, ggsave, path = here::here("plots")) 
}

```

